{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment Analysis - Modeling\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier,RandomForestClassifier\n",
    "\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in c:\\users\\leanne\\anaconda3\\lib\\site-packages (0.13.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "---\n",
    "\n",
    "In this segment, we will use the following models to classify the tweets. \n",
    "\n",
    "##### Models Used: \n",
    "1. Logistic Regression Model (??)\n",
    "2. Multinomial Naive Bayes Model \n",
    "3. Decision Tree\n",
    "3. Use of Grid Search to optimise the number of features in the count vectoriser in an attempt to improve model accuracy\n",
    "\n",
    "##### Metric to Validate Model: \n",
    "\n",
    "Accuracy is likely the best metric to use here as improperly classifying a subreddit post is equally bad in this instance.\n",
    "\n",
    "##### Outcome:\n",
    "\n",
    "The Multinomial Naive Bayes Model was able to accurately classify 91.8% of the posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../datasets/tweets_clean_1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>said</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plu ad commerci experi tacki</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>today must mean need take anoth trip</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>realli aggress blast obnoxi entertain guest fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>realli big bad thing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  airline_sentiment\n",
       "0                                               said                  1\n",
       "1                       plu ad commerci experi tacki                  2\n",
       "2               today must mean need take anoth trip                  1\n",
       "3  realli aggress blast obnoxi entertain guest fa...                  0\n",
       "4                               realli big bad thing                  0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling \n",
    "\n",
    "General Approach\n",
    "- Split data into X and y \n",
    "- Train test split for model validation\n",
    "- Hyperparameter optimisation for count vectoriser, Tfidf transformer and model hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split data into `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_1['text']\n",
    "y = df_1['airline_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                 said\n",
       "1                         plu ad commerci experi tacki\n",
       "2                 today must mean need take anoth trip\n",
       "3    realli aggress blast obnoxi entertain guest fa...\n",
       "4                                 realli big bad thing\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.627149\n",
       "1    0.211698\n",
       "2    0.161153\n",
       "Name: airline_sentiment, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function for model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_type, X,y):\n",
    "    \n",
    "    #train test split data for model validation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify = y)\n",
    "\n",
    "    #specifying models\n",
    "    models = {'lr': LogisticRegression(), \n",
    "              'nb': MultinomialNB(),\n",
    "              'dt': DecisionTreeClassifier(),\n",
    "              'rf': RandomForestClassifier(),\n",
    "              'ada': AdaBoostClassifier(base_estimator=DecisionTreeClassifier()),\n",
    "              'gb': GradientBoostingClassifier()\n",
    "             }\n",
    "    \n",
    "    #creating a pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('cvec', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        (model_type, models[model_type])\n",
    "    ])\n",
    "    \n",
    "    #pipeline parameters\n",
    "    pipe_params = {\n",
    "    'cvec__max_df': (.9, .8),\n",
    "    'cvec__max_features':[2500,5000,7000],\n",
    "    'cvec__max_df':[0.9,0.95],\n",
    "    'cvec__ngram_range':[(1,1),(1,2)],\n",
    "    'tfidf__use_idf':[True,False],\n",
    "\n",
    "    }\n",
    "        \n",
    "    #additional parameters for each model \n",
    "    if model_type == 'nb':\n",
    "        pipe_params.update({'nb__alpha':[1,2]\n",
    "                           })\n",
    "        \n",
    "    elif model_type == 'lr':\n",
    "        pipe_params.update({'lr__penalty':['none','l2'],\n",
    "                            'lr__max_iter':[1000]\n",
    "                           })\n",
    "        \n",
    "    elif model_type == 'dt':\n",
    "        pipe_params.update({'dt__max_depth':[3,5,10],\n",
    "                           'dt__min_samples_split':[5,10,20],\n",
    "                           'dt__min_samples_leaf':[2,5,7]\n",
    "                           })\n",
    "    \n",
    "    elif model_type == 'rf':\n",
    "        pipe_params.update({'rf__n_estimators': [100,150,200],\n",
    "                            'rf__max_depth':[None,1,2,3,4,5]\n",
    "                           })\n",
    "    elif model_type == 'ada':\n",
    "        pipe_params.update({'ada__n_estimators':[50,100],\n",
    "                            'ada__learning_rate':[0.9,1],\n",
    "                            'ada__base_estimator__max_depth':[1,2,3]\n",
    "                           })\n",
    "    elif model_type == 'gb':\n",
    "        pipe_params.update({'gb__n_estimators': [50,100,150],\n",
    "                            'gb__learning_rate':[0.08, 0.1, 0.12],\n",
    "                            'gb__max_depth':[1,2,3]\n",
    "        })\n",
    "\n",
    "\n",
    "    #grid search \n",
    "    gs = GridSearchCV(estimator = pipe, \n",
    "                        param_grid = pipe_params,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1, \n",
    "                        cv = 5)\n",
    "    \n",
    "    print(\"Grid Search for \" + model_type)\n",
    "    print('===========================================================================')\n",
    "    \n",
    "    gs.fit(X_train,y_train)\n",
    "\n",
    "    print(f\"Best Score: {gs.best_score_}\")\n",
    "    print(f\"Best Parameters: {gs.best_params_}\")\n",
    "    \n",
    "    #train model on entire train data set using  best parameters\n",
    "    pipeline_final= Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    (model_type, models[model_type])\n",
    "    ])\n",
    "    \n",
    "    pipeline_final.set_params(**gs.best_params_)\n",
    "    final_model = pipeline_final.fit(X_train,y_train)\n",
    "    \n",
    "    print(\"Final Model Score\")\n",
    "    print()\n",
    "    print(f'Train Score:{final_model.score(X_train,y_train)}')\n",
    "    print(f'Test Score:{final_model.score(X_test,y_test)}')\n",
    "    \n",
    "    #save model with best parameters for use later\n",
    "    filename = 'model_' + model_type +'.pkl'\n",
    "    joblib.dump(final_model,filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search for lr\n",
      "===========================================================================\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   21.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7764383561643836\n",
      "Best Parameters: {'cvec__max_df': 0.9, 'cvec__max_features': 5000, 'cvec__ngram_range': (1, 2), 'lr__max_iter': 1000, 'lr__penalty': 'l2', 'tfidf__use_idf': False}\n",
      "Final Model Score\n",
      "\n",
      "Train Score:0.848310502283105\n",
      "Test Score:0.7825253355245139\n"
     ]
    }
   ],
   "source": [
    "run_model('lr',X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search for nb\n",
      "===========================================================================\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:   16.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7345205479452054\n",
      "Best Parameters: {'cvec__max_df': 0.9, 'cvec__max_features': 2500, 'cvec__ngram_range': (1, 2), 'nb__alpha': 1, 'tfidf__use_idf': True}\n",
      "Final Model Score\n",
      "\n",
      "Train Score:0.78\n",
      "Test Score:0.7376061353053958\n"
     ]
    }
   ],
   "source": [
    "run_model('nb',X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search for dt\n",
      "===========================================================================\n",
      "Fitting 5 folds for each of 648 candidates, totalling 3240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   47.4s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3240 out of 3240 | elapsed:  7.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.6925114155251142\n",
      "Best Parameters: {'cvec__max_df': 0.9, 'cvec__max_features': 5000, 'cvec__ngram_range': (1, 2), 'dt__max_depth': 10, 'dt__min_samples_leaf': 2, 'dt__min_samples_split': 5, 'tfidf__use_idf': False}\n",
      "Final Model Score\n",
      "\n",
      "Train Score:0.7052054794520548\n",
      "Test Score:0.6894001643385373\n"
     ]
    }
   ],
   "source": [
    "run_model('dt',X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search for rf\n",
      "===========================================================================\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 12.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 19.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 24.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2160 out of 2160 | elapsed: 28.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.76337899543379\n",
      "Best Parameters: {'cvec__max_df': 0.9, 'cvec__max_features': 5000, 'cvec__ngram_range': (1, 1), 'rf__max_depth': None, 'rf__n_estimators': 200, 'tfidf__use_idf': True}\n",
      "Final Model Score\n",
      "\n",
      "Train Score:0.9926027397260274\n",
      "Test Score:0.7636264037250069\n"
     ]
    }
   ],
   "source": [
    "run_model('rf',X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search for ada\n",
      "===========================================================================\n",
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed: 12.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7262100456621006\n",
      "Best Parameters: {'ada__base_estimator__max_depth': 2, 'ada__learning_rate': 0.9, 'ada__n_estimators': 100, 'cvec__max_df': 0.9, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 2), 'tfidf__use_idf': False}\n",
      "Final Model Score\n",
      "\n",
      "Train Score:0.7658447488584474\n",
      "Test Score:0.7192549986305122\n"
     ]
    }
   ],
   "source": [
    "run_model('ada',X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search for gb\n",
      "===========================================================================\n",
      "Fitting 5 folds for each of 648 candidates, totalling 3240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   23.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 15.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 27.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 43.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 60.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 83.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3240 out of 3240 | elapsed: 85.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7415525114155251\n",
      "Best Parameters: {'cvec__max_df': 0.95, 'cvec__max_features': 5000, 'cvec__ngram_range': (1, 1), 'gb__learning_rate': 0.12, 'gb__max_depth': 3, 'gb__n_estimators': 150, 'tfidf__use_idf': False}\n",
      "Final Model Score\n",
      "\n",
      "Train Score:0.7770776255707763\n",
      "Test Score:0.7348671596822788\n"
     ]
    }
   ],
   "source": [
    "run_model('gb',X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                    | Accuracy Score |\n",
    "|--------------------------|----------------|\n",
    "| Logistic Regression      | 0\\.782         |\n",
    "| Decision Tree            | 0\\.737         |\n",
    "| Random Forest Classifier | 0\\.689         |\n",
    "| Ada Boost                | 0\\.763         |\n",
    "| Gradient Boost           | 0\\.719         |\n",
    "| Support Vector Machine   | 0\\.734         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'lr': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = joblib.load(filename)\n",
    "# result = loaded_model.score(X_test, Y_test)\n",
    "# print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
